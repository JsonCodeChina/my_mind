#!/usr/bin/env python3
"""
Claude Code æ•°æ®é‡‡é›†è„šæœ¬ V2 (ä¼˜åŒ–ç‰ˆ)

æ–°åŠŸèƒ½ï¼š
1. æŠ“å– GitHub Issue é«˜èµè¯„è®º
2. æŠ“å– HN è®¨è®ºè¯„è®º
3. æ•°æ®å­˜å…¥ SQLite æ•°æ®åº“
4. è¶‹åŠ¿åˆ†æï¼ˆåŸºäºå†å²æ•°æ®ï¼‰
5. æ™ºèƒ½è¯„è®ºç­›é€‰å’Œæ‰“åˆ†

ä½¿ç”¨ï¼š
    python fetch_data_v2.py [--output OUTPUT_PATH] [--days LOOKBACK_DAYS]

ä¾èµ–ï¼š
    pip install requests beautifulsoup4 python-dateutil
"""

import argparse
import json
import logging
import os
import re
import sys
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional
from urllib.parse import urljoin

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../..', '.shared/utils'))
from db_manager import DatabaseManager, predict_resolution_time

try:
    import requests
    from bs4 import BeautifulSoup
    from dateutil import parser as date_parser
except ImportError as e:
    print(f"é”™è¯¯: ç¼ºå°‘å¿…éœ€çš„ä¾èµ–åº“ - {e}")
    print("è¯·è¿è¡Œ: pip install requests beautifulsoup4 python-dateutil")
    sys.exit(1)


# ============================================================================
# é…ç½®
# ============================================================================

class Config:
    """é…ç½®å¸¸é‡"""
    # API URLs
    GITHUB_API_BASE = "https://api.github.com"
    GITHUB_REPO = "anthropics/claude-code"
    HN_ALGOLIA_API = "https://hn.algolia.com/api/v1/search"
    HN_ITEM_API = "https://hacker-news.firebaseio.com/v0/item"
    CHANGELOG_URL = "https://claudelog.com/claude-code-changelog/"

    # é»˜è®¤å‚æ•°
    DEFAULT_OUTPUT_PATH = "/Users/shenbo/Desktop/mind/cc/.meta/cache/daily_data.json"
    DEFAULT_GITHUB_LOOKBACK_DAYS = 3
    DEFAULT_HN_LOOKBACK_DAYS = 7

    # ç­›é€‰é˜ˆå€¼
    ISSUE_HEAT_THRESHOLD = 30
    HN_HEAT_THRESHOLD = 70

    # æ•°é‡é™åˆ¶
    MAX_ISSUES = 3
    MAX_DISCUSSIONS = 2
    MAX_COMMENTS_PER_ISSUE = 10  # æ¯ä¸ª Issue æœ€å¤šæŠ“å–å¤šå°‘è¯„è®º

    # è¯·æ±‚è¶…æ—¶
    REQUEST_TIMEOUT = 10

    # User-Agent
    USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"


# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def make_request(url: str, params: Optional[Dict] = None,
                 headers: Optional[Dict] = None) -> Optional[requests.Response]:
    """å‘é€ HTTP è¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    default_headers = {"User-Agent": Config.USER_AGENT}
    if headers:
        default_headers.update(headers)

    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = requests.get(
                url,
                params=params,
                headers=default_headers,
                timeout=Config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                logger.error(f"è¯·æ±‚æœ€ç»ˆå¤±è´¥: {url}")
                return None

    return None


def parse_date(date_string: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
    try:
        return date_parser.parse(date_string)
    except Exception:
        return None


# ============================================================================
# çƒ­åº¦è¯„åˆ†ç®—æ³•
# ============================================================================

def calculate_issue_heat_score(issue_data: Dict) -> float:
    """è®¡ç®— GitHub Issue çƒ­åº¦åˆ†æ•°"""
    comments = issue_data.get('comments', 0)
    reactions = issue_data.get('reactions', {}).get('total_count', 0)

    # åŸºç¡€åˆ†æ•°
    base_score = (comments * 2) + (reactions * 1.5)

    # æ—¶é—´åŠ æˆ
    created_at = parse_date(issue_data.get('created_at', ''))
    time_bonus = 0
    if created_at:
        hours_old = (datetime.now(timezone.utc) - created_at).total_seconds() / 3600
        if hours_old < 24:
            time_bonus = 20
        elif hours_old < 72:
            time_bonus = 10

    # æ ‡ç­¾åŠ æˆ
    label_bonus = 0
    priority_labels = {
        'bug': 5,
        'feature': 3,
        'has repro': 5,
        'priority-high': 10,
        'priority-critical': 15
    }

    for label in issue_data.get('labels', []):
        label_name = label.get('name', '')
        if label_name in priority_labels:
            label_bonus += priority_labels[label_name]

    heat_score = base_score + time_bonus + label_bonus
    return round(heat_score, 2)


def calculate_hn_heat_score(story_data: Dict) -> float:
    """è®¡ç®— HN è®¨è®ºçƒ­åº¦åˆ†æ•°"""
    points = story_data.get('points', 0)
    comments = story_data.get('num_comments', 0)

    # åŸºç¡€åˆ†æ•°
    base_score = (points * 1.5) + (comments * 2)

    # æ—¶é—´åŠ æˆ
    created_timestamp = story_data.get('created_at_i', 0)
    time_bonus = 0
    if created_timestamp:
        days_old = (datetime.now(timezone.utc).timestamp() - created_timestamp) / 86400
        if days_old < 1:
            time_bonus = 30
        elif days_old < 3:
            time_bonus = 15
        elif days_old < 7:
            time_bonus = 5

    heat_score = base_score + time_bonus
    return round(heat_score, 2)


# ============================================================================
# è¯„è®ºæŠ“å–å’Œç­›é€‰
# ============================================================================

class CommentFetcher:
    """è¯„è®ºæŠ“å–å’Œæ™ºèƒ½ç­›é€‰å™¨"""

    @staticmethod
    def fetch_github_comments(issue_number: int) -> List[Dict]:
        """æŠ“å– GitHub Issue çš„è¯„è®º"""
        logger.info(f"  â†’ è·å– Issue #{issue_number} çš„è¯„è®º...")

        url = f"{Config.GITHUB_API_BASE}/repos/{Config.GITHUB_REPO}/issues/{issue_number}/comments"
        params = {
            "per_page": 100,
            "sort": "created",
            "direction": "desc"
        }

        response = make_request(url, params=params)
        if not response:
            logger.warning(f"    è·å–è¯„è®ºå¤±è´¥: Issue #{issue_number}")
            return []

        try:
            comments_raw = response.json()

            # å¦‚æœæ²¡æœ‰è¯„è®ºï¼Œç›´æ¥è¿”å›
            if not comments_raw:
                logger.info(f"    Issue #{issue_number} æ— è¯„è®º")
                return []

            comments = []
            for comment in comments_raw:
                comment_data = {
                    "author": comment['user']['login'],
                    "body": comment['body'][:1000],  # é™åˆ¶é•¿åº¦
                    "upvotes": comment['reactions']['+1'],
                    "created_at": comment['created_at'],
                    "is_solution": CommentFetcher.detect_solution(comment['body']),
                    "is_interesting": CommentFetcher.detect_interesting(comment['body'])
                }

                # è®¡ç®—è´¨é‡åˆ†æ•°
                comment_data['quality_score'] = CommentFetcher.calculate_comment_quality(comment_data)

                comments.append(comment_data)

            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            comments.sort(key=lambda x: x['quality_score'], reverse=True)

            # åªä¿ç•™é«˜è´¨é‡è¯„è®º
            top_comments = [c for c in comments if c['quality_score'] > 10][:Config.MAX_COMMENTS_PER_ISSUE]

            logger.info(f"    âœ“ è·å–åˆ° {len(comments)} æ¡è¯„è®ºï¼Œç­›é€‰å‡º {len(top_comments)} æ¡é«˜è´¨é‡è¯„è®º")
            return top_comments

        except Exception as e:
            logger.error(f"è§£æè¯„è®ºå¤±è´¥: {e}")
            return []

    @staticmethod
    def detect_solution(text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯è§£å†³æ–¹æ¡ˆ"""
        if not text:
            return False

        solution_keywords = [
            "workaround", "solution", "fix", "solved", "works for me",
            "ä¸´æ—¶æ–¹æ¡ˆ", "è§£å†³æ–¹æ³•", "ä¿®å¤", "å¯ä»¥ç”¨", "æœ‰æ•ˆ"
        ]

        text_lower = text.lower()
        return any(kw in text_lower for kw in solution_keywords)

    @staticmethod
    def detect_interesting(text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰è¶£/æœ‰ä»·å€¼"""
        if not text or len(text) < 80:
            return False

        # åŒ…å«ä»£ç å—
        if "```" in text or "`" in text:
            return True

        # åŒ…å«é“¾æ¥ï¼ˆå¯èƒ½æ˜¯å‚è€ƒèµ„æ–™ï¼‰
        if "http" in text:
            return True

        # åŒ…å«æŠ€æœ¯å…³é”®è¯
        tech_keywords = [
            "API", "error", "debug", "code", "function", "implementation",
            "architecture", "performance", "optimization"
        ]

        text_lower = text.lower()
        keyword_count = sum(1 for kw in tech_keywords if kw.lower() in text_lower)

        return keyword_count >= 2

    @staticmethod
    def calculate_comment_quality(comment_data: Dict) -> float:
        """è®¡ç®—è¯„è®ºè´¨é‡åˆ†æ•°"""
        score = 0

        # ç‚¹èµæ•°æƒé‡æœ€é«˜
        score += comment_data['upvotes'] * 3

        # é•¿åº¦åŠ åˆ†ï¼ˆä½†ä¸è¦å¤ªé•¿ï¼‰
        text_length = len(comment_data['body'])
        if 100 < text_length < 500:
            score += 10
        elif 500 <= text_length < 1000:
            score += 5

        # æ˜¯è§£å†³æ–¹æ¡ˆ
        if comment_data['is_solution']:
            score += 30

        # æœ‰è¶£/æœ‰ä»·å€¼
        if comment_data['is_interesting']:
            score += 15

        return round(score, 2)


# ============================================================================
# æ•°æ®é‡‡é›†
# ============================================================================

def fetch_github_issues(lookback_days: int, db: DatabaseManager) -> List[Dict]:
    """ä» GitHub API è·å–æœ€è¿‘çš„ Issues"""
    logger.info("æ­£åœ¨è·å– GitHub Issues...")

    since_date = (datetime.now(timezone.utc) - timedelta(days=lookback_days)).isoformat()
    url = f"{Config.GITHUB_API_BASE}/repos/{Config.GITHUB_REPO}/issues"
    params = {
        'state': 'all',
        'sort': 'updated',
        'direction': 'desc',
        'per_page': 30,
        'since': since_date
    }

    response = make_request(url, params=params)
    if not response:
        logger.error("GitHub API è¯·æ±‚å¤±è´¥")
        return []

    try:
        issues_raw = response.json()
        issues = []

        for issue in issues_raw:
            # è·³è¿‡ Pull Requests
            if 'pull_request' in issue:
                continue

            issue_data = {
                'number': issue['number'],
                'title': issue['title'],
                'url': issue['html_url'],
                'state': issue['state'],
                'comments': issue['comments'],
                'reactions': issue['reactions'],
                'labels': [{'name': label['name']} for label in issue['labels']],
                'created_at': issue['created_at'],
                'updated_at': issue['updated_at']
            }

            # è®¡ç®—çƒ­åº¦åˆ†æ•°
            issue_data['heat_score'] = calculate_issue_heat_score(issue)

            # è¿‡æ»¤ä½çƒ­åº¦ Issues
            if issue_data['heat_score'] >= Config.ISSUE_HEAT_THRESHOLD:
                # è·å–è¯„è®º
                top_comments = CommentFetcher.fetch_github_comments(issue_data['number'])
                issue_data['top_comments'] = top_comments

                # è·å–è¶‹åŠ¿æ•°æ®
                trend_data = db.get_issue_trend(issue_data['number'], days=7)
                issue_data['trend'] = trend_data

                # é¢„æµ‹ä¿®å¤æ—¶é—´
                if trend_data:
                    issue_data['prediction'] = predict_resolution_time(
                        trend_data.get('comment_rate', 0),
                        issue_data['heat_score'],
                        issue_data['state']
                    )

                # å­˜å…¥æ•°æ®åº“
                db.insert_issue(issue_data)
                for comment in top_comments:
                    db.insert_comment(issue_data['number'], comment)

                issues.append(issue_data)

        # æŒ‰çƒ­åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        issues.sort(key=lambda x: x['heat_score'], reverse=True)
        issues = issues[:Config.MAX_ISSUES]

        logger.info(f"âœ“ è·å–åˆ° {len(issues)} ä¸ªçƒ­é—¨ Issues")
        return issues

    except Exception as e:
        logger.error(f"è§£æ GitHub æ•°æ®å¤±è´¥: {e}")
        return []


def fetch_hn_discussions(lookback_days: int, db: DatabaseManager) -> List[Dict]:
    """ä» HN Algolia API è·å–ç›¸å…³è®¨è®º"""
    logger.info("æ­£åœ¨è·å– Hacker News è®¨è®º...")

    since_timestamp = int((datetime.now(timezone.utc) - timedelta(days=lookback_days)).timestamp())
    params = {
        'query': 'Claude Code',
        'tags': 'story',
        'numericFilters': f'created_at_i>{since_timestamp}'
    }

    response = make_request(Config.HN_ALGOLIA_API, params=params)
    if not response:
        logger.error("HN API è¯·æ±‚å¤±è´¥")
        return []

    try:
        data = response.json()
        discussions = []

        for story in data.get('hits', []):
            discussion_data = {
                'objectID': story['objectID'],
                'title': story['title'],
                'url': story.get('url', ''),
                'hn_url': f"https://news.ycombinator.com/item?id={story['objectID']}",
                'points': story.get('points', 0),
                'comments': story.get('num_comments', 0),
                'author': story.get('author', ''),
                'created_at': story.get('created_at', '')
            }

            # è®¡ç®—çƒ­åº¦åˆ†æ•°
            discussion_data['heat_score'] = calculate_hn_heat_score(story)

            # è¿‡æ»¤ä½çƒ­åº¦è®¨è®º
            if discussion_data['heat_score'] >= Config.HN_HEAT_THRESHOLD:
                # å­˜å…¥æ•°æ®åº“
                db.insert_discussion(discussion_data)
                discussions.append(discussion_data)

        # æŒ‰çƒ­åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
        discussions.sort(key=lambda x: x['heat_score'], reverse=True)
        discussions = discussions[:Config.MAX_DISCUSSIONS]

        logger.info(f"âœ“ è·å–åˆ° {len(discussions)} ä¸ªçƒ­é—¨è®¨è®º")
        return discussions

    except Exception as e:
        logger.error(f"è§£æ HN æ•°æ®å¤±è´¥: {e}")
        return []


def fetch_version_info(db: DatabaseManager) -> Dict:
    """ä» changelog é¡µé¢æå–ç‰ˆæœ¬ä¿¡æ¯"""
    logger.info("æ­£åœ¨æ£€æµ‹ç‰ˆæœ¬ä¿¡æ¯...")

    response = make_request(Config.CHANGELOG_URL)
    if not response:
        logger.error("Changelog è¯·æ±‚å¤±è´¥")
        return {
            'current': 'unknown',
            'release_date': datetime.now().strftime('%Y-%m-%d'),
            'is_new': False,
            'changelog_url': Config.CHANGELOG_URL,
            'changes': []
        }

    try:
        soup = BeautifulSoup(response.content, 'html.parser')
        version_pattern = re.compile(r'v?\d+\.\d+\.\d+')

        page_text = soup.get_text()
        all_versions = version_pattern.findall(page_text)

        current_version = 'unknown'
        release_date = datetime.now().strftime('%Y-%m-%d')

        if all_versions:
            current_version = all_versions[0]
            if not current_version.startswith('v'):
                current_version = f'v{current_version}'

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°ç‰ˆæœ¬
        latest_in_db = db.get_latest_version('claude-code')
        is_new = False

        if latest_in_db:
            is_new = current_version != latest_in_db['version']
            if is_new:
                logger.info(f"ğŸ†• æ£€æµ‹åˆ°æ–°ç‰ˆæœ¬: {latest_in_db['version']} â†’ {current_version}")
                db.insert_version('claude-code', current_version, release_date)
        else:
            # é¦–æ¬¡è¿è¡Œï¼Œæ’å…¥å½“å‰ç‰ˆæœ¬
            db.insert_version('claude-code', current_version, release_date)

        logger.info(f"âœ“ å½“å‰ç‰ˆæœ¬: {current_version}")

        return {
            'current': current_version,
            'release_date': release_date,
            'is_new': is_new,
            'changelog_url': Config.CHANGELOG_URL,
            'changes': []
        }

    except Exception as e:
        logger.error(f"è§£æç‰ˆæœ¬ä¿¡æ¯å¤±è´¥: {e}")
        return {
            'current': 'unknown',
            'release_date': datetime.now().strftime('%Y-%m-%d'),
            'is_new': False,
            'changelog_url': Config.CHANGELOG_URL,
            'changes': []
        }


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Claude Code æ•°æ®é‡‡é›†è„šæœ¬ V2')
    parser.add_argument('--output', '-o', default=Config.DEFAULT_OUTPUT_PATH,
                        help=f'è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: {Config.DEFAULT_OUTPUT_PATH})')
    parser.add_argument('--github-days', type=int, default=Config.DEFAULT_GITHUB_LOOKBACK_DAYS,
                        help=f'GitHub Issues å›æº¯å¤©æ•° (é»˜è®¤: {Config.DEFAULT_GITHUB_LOOKBACK_DAYS})')
    parser.add_argument('--hn-days', type=int, default=Config.DEFAULT_HN_LOOKBACK_DAYS,
                        help=f'HN è®¨è®ºå›æº¯å¤©æ•° (é»˜è®¤: {Config.DEFAULT_HN_LOOKBACK_DAYS})')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(logging.DEBUG)

    logger.info("=" * 60)
    logger.info("Claude Code æ•°æ®é‡‡é›†è„šæœ¬ V2 (ä¼˜åŒ–ç‰ˆ)")
    logger.info("=" * 60)

    # åˆå§‹åŒ–æ•°æ®åº“
    db = DatabaseManager()

    # é‡‡é›†æ•°æ®
    start_time = datetime.now()

    issues = fetch_github_issues(args.github_days, db)
    discussions = fetch_hn_discussions(args.hn_days, db)
    version = fetch_version_info(db)

    # æ„å»ºè¾“å‡ºæ•°æ®
    output_data = {
        'metadata': {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'date': datetime.now().strftime('%Y-%m-%d'),
            'version': '2.0'
        },
        'version': version,
        'issues': issues,
        'discussions': discussions,
        'articles': []
    }

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    # å†™å…¥æ–‡ä»¶
    try:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ æ•°æ®å·²ä¿å­˜åˆ°: {args.output}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return 1

    # ç»Ÿè®¡ä¿¡æ¯
    elapsed_time = (datetime.now() - start_time).total_seconds()
    total_comments = sum(len(issue.get('top_comments', [])) for issue in issues)

    logger.info("=" * 60)
    logger.info("é‡‡é›†å®Œæˆï¼")
    logger.info(f"  Issues: {len(issues)} ä¸ª")
    logger.info(f"  è¯„è®º: {total_comments} æ¡")
    logger.info(f"  HN è®¨è®º: {len(discussions)} ä¸ª")
    logger.info(f"  ç‰ˆæœ¬: {version['current']}")
    logger.info(f"  è€—æ—¶: {elapsed_time:.2f} ç§’")
    logger.info("=" * 60)

    return 0


if __name__ == '__main__':
    sys.exit(main())
